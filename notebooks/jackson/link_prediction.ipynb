{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\rwn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid, Actor\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.utils import to_networkx\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "def remove_edges(G, edges):\n",
    "    G_new = deepcopy(G)\n",
    "    G_new.remove_edges_from(edges)\n",
    "    return G_new\n",
    "\n",
    "def create_train_test_split_edge(data):\n",
    "    data = cora[0]\n",
    "\n",
    "    # Create a list of positive and negative edges\n",
    "    u, v = data.edge_index.numpy()\n",
    "\n",
    "    adj = coo_matrix((np.ones(data.num_edges), data.edge_index.numpy()))\n",
    "    adj_neg = 1 - adj.todense() - np.eye(data.num_nodes)\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    # Create train/test edge split\n",
    "    test_size = int(np.floor(data.num_edges * 0.1))\n",
    "    eids = np.random.permutation(np.arange(data.num_edges)) # Create an array of 'edge IDs'\n",
    "\n",
    "    train_pos_u, train_pos_v = data.edge_index[:, eids[test_size:]]\n",
    "    test_pos_u, test_pos_v   = data.edge_index[:, eids[:test_size]]\n",
    "\n",
    "    # Sample an equal amount of negative edges from  the graph, split into train/test\n",
    "    neg_eids = np.random.choice(len(neg_u), data.num_edges)\n",
    "    test_neg_u, test_neg_v = (\n",
    "        neg_u[neg_eids[:test_size]],\n",
    "        neg_v[neg_eids[:test_size]],\n",
    "    )\n",
    "    train_neg_u, train_neg_v = (\n",
    "        neg_u[neg_eids[test_size:]],\n",
    "        neg_v[neg_eids[test_size:]],\n",
    "    )\n",
    "\n",
    "    # Remove test edges from original graph\n",
    "    G = to_networkx(data, node_attrs=data.node_attrs(), to_undirected=data.is_undirected())\n",
    "    G_train = remove_edges(G, np.column_stack([test_pos_u, test_pos_v])) \n",
    "\n",
    "    train_g = dgl.from_networkx(G_train, node_attrs=list(G.nodes[0].keys()))\n",
    "\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=data.num_nodes)\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=data.num_nodes)\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=data.num_nodes)\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=data.num_nodes)\n",
    "\n",
    "    return train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, \"mean\")\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "class DotPredictor(torch.nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata[\"score\"][:, 0]\n",
    "        \n",
    "\n",
    "class MLPPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_feats):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_feats, 32)\n",
    "        self.linear2 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(fn.u_add_v(\"h\", \"h\", \"score\"))\n",
    "\n",
    "            score = g.edata['score']\n",
    "\n",
    "            score = self.linear1(score)\n",
    "            score = F.relu(score)\n",
    "            score = self.linear2(score)\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return score.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora model:\n",
    "train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = create_train_test_split_edge(cora[0])\n",
    "\n",
    "model = GraphSAGE(train_g.ndata[\"x\"].shape[1], 32)\n",
    "pred = MLPPredictor(32)\n",
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6941514015197754\n",
      "AUC 0.4900258305069518\n",
      "In epoch 5, loss: 0.6813181638717651\n",
      "In epoch 10, loss: 0.6224610805511475\n",
      "In epoch 15, loss: 0.6055197715759277\n",
      "In epoch 20, loss: 0.5536144971847534\n",
      "In epoch 25, loss: 0.5455098748207092\n",
      "In epoch 30, loss: 0.5327537059783936\n",
      "In epoch 35, loss: 0.5170523524284363\n",
      "In epoch 40, loss: 0.5034334063529968\n",
      "In epoch 45, loss: 0.487308144569397\n",
      "In epoch 50, loss: 0.46476295590400696\n",
      "In epoch 55, loss: 0.4387530982494354\n",
      "In epoch 60, loss: 0.4108203947544098\n",
      "In epoch 65, loss: 0.3884579837322235\n",
      "In epoch 70, loss: 0.36885544657707214\n",
      "In epoch 75, loss: 0.34917932748794556\n",
      "In epoch 80, loss: 0.3297191262245178\n",
      "In epoch 85, loss: 0.3122040927410126\n",
      "In epoch 90, loss: 0.2994556725025177\n",
      "In epoch 95, loss: 0.28140905499458313\n",
      "In epoch 100, loss: 0.2747761607170105\n",
      "AUC 0.7697859437119562\n",
      "In epoch 105, loss: 0.25600486993789673\n",
      "In epoch 110, loss: 0.24686764180660248\n",
      "In epoch 115, loss: 0.2364881932735443\n",
      "In epoch 120, loss: 0.22714293003082275\n",
      "In epoch 125, loss: 0.22680287063121796\n",
      "In epoch 130, loss: 0.21010001003742218\n",
      "In epoch 135, loss: 0.20365290343761444\n",
      "In epoch 140, loss: 0.19502446055412292\n",
      "In epoch 145, loss: 0.1941520720720291\n",
      "In epoch 150, loss: 0.18235863745212555\n",
      "In epoch 155, loss: 0.19781452417373657\n",
      "In epoch 160, loss: 0.18258196115493774\n",
      "In epoch 165, loss: 0.1719736009836197\n",
      "In epoch 170, loss: 0.1658112108707428\n",
      "In epoch 175, loss: 0.1605503410100937\n",
      "In epoch 180, loss: 0.15176159143447876\n",
      "In epoch 185, loss: 0.15688347816467285\n",
      "In epoch 190, loss: 0.1463455706834793\n",
      "In epoch 195, loss: 0.14354997873306274\n",
      "In epoch 200, loss: 0.13860993087291718\n",
      "AUC 0.8166492217155947\n",
      "In epoch 205, loss: 0.13200832903385162\n",
      "In epoch 210, loss: 0.12984509766101837\n",
      "In epoch 215, loss: 0.1242709830403328\n",
      "In epoch 220, loss: 0.1197538673877716\n",
      "In epoch 225, loss: 0.11796998977661133\n",
      "In epoch 230, loss: 0.2218969315290451\n",
      "In epoch 235, loss: 0.14444778859615326\n",
      "In epoch 240, loss: 0.14616967737674713\n",
      "In epoch 245, loss: 0.11772355437278748\n",
      "In epoch 250, loss: 0.1119944155216217\n",
      "In epoch 255, loss: 0.10729462653398514\n",
      "In epoch 260, loss: 0.10137366503477097\n",
      "In epoch 265, loss: 0.09553741663694382\n",
      "In epoch 270, loss: 0.09290570020675659\n",
      "In epoch 275, loss: 0.08887697011232376\n",
      "In epoch 280, loss: 0.08593416213989258\n",
      "In epoch 285, loss: 0.08305028080940247\n",
      "In epoch 290, loss: 0.08029144257307053\n",
      "In epoch 295, loss: 0.078031025826931\n",
      "In epoch 300, loss: 0.0756201297044754\n",
      "AUC 0.8279517531052761\n",
      "In epoch 305, loss: 0.07332585752010345\n",
      "In epoch 310, loss: 0.07350734621286392\n",
      "In epoch 315, loss: 0.06926911324262619\n",
      "In epoch 320, loss: 0.06858821213245392\n",
      "In epoch 325, loss: 0.06724663078784943\n",
      "In epoch 330, loss: 0.06357765942811966\n",
      "In epoch 335, loss: 0.06381115317344666\n",
      "In epoch 340, loss: 0.06090090051293373\n",
      "In epoch 345, loss: 0.061968594789505005\n",
      "In epoch 350, loss: 0.05739494040608406\n",
      "In epoch 355, loss: 0.05730423331260681\n",
      "In epoch 360, loss: 0.055029407143592834\n",
      "In epoch 365, loss: 0.05462566763162613\n",
      "In epoch 370, loss: 0.05240391194820404\n",
      "In epoch 375, loss: 0.05184487625956535\n",
      "In epoch 380, loss: 0.05022692680358887\n",
      "In epoch 385, loss: 0.048025574535131454\n",
      "In epoch 390, loss: 0.05034217983484268\n",
      "In epoch 395, loss: 0.04483243077993393\n",
      "In epoch 400, loss: 0.04386648163199425\n",
      "AUC 0.8337809123784282\n",
      "In epoch 405, loss: 0.04350195452570915\n",
      "In epoch 410, loss: 0.040474385023117065\n",
      "In epoch 415, loss: 0.04251938685774803\n",
      "In epoch 420, loss: 0.03921351954340935\n",
      "In epoch 425, loss: 0.03729327768087387\n",
      "In epoch 430, loss: 0.036997176706790924\n",
      "In epoch 435, loss: 0.0356271006166935\n",
      "In epoch 440, loss: 0.03362734615802765\n",
      "In epoch 445, loss: 0.03449160233139992\n",
      "In epoch 450, loss: 0.032144695520401\n",
      "In epoch 455, loss: 0.031144702807068825\n",
      "In epoch 460, loss: 0.030978888273239136\n",
      "In epoch 465, loss: 0.02989838644862175\n",
      "In epoch 470, loss: 0.028396598994731903\n",
      "In epoch 475, loss: 0.027223750948905945\n",
      "In epoch 480, loss: 0.027297399938106537\n",
      "In epoch 485, loss: 0.026458976790308952\n",
      "In epoch 490, loss: 0.0250035859644413\n",
      "In epoch 495, loss: 0.024648133665323257\n",
      "In epoch 500, loss: 0.02417689934372902\n",
      "AUC 0.8385921250645761\n",
      "In epoch 505, loss: 0.023020796477794647\n",
      "In epoch 510, loss: 0.022909589111804962\n",
      "In epoch 515, loss: 0.022340573370456696\n",
      "In epoch 520, loss: 0.021327828988432884\n",
      "In epoch 525, loss: 0.021131137385964394\n",
      "In epoch 530, loss: 0.020618785172700882\n",
      "In epoch 535, loss: 0.019924581050872803\n",
      "In epoch 540, loss: 0.01980515941977501\n",
      "In epoch 545, loss: 0.019491590559482574\n",
      "In epoch 550, loss: 0.018656078726053238\n",
      "In epoch 555, loss: 0.018329642713069916\n",
      "In epoch 560, loss: 0.01824556663632393\n",
      "In epoch 565, loss: 0.017795994877815247\n",
      "In epoch 570, loss: 0.017186298966407776\n",
      "In epoch 575, loss: 0.01677396520972252\n",
      "In epoch 580, loss: 0.016512159258127213\n",
      "In epoch 585, loss: 0.01607508398592472\n",
      "In epoch 590, loss: 0.015585302375257015\n",
      "In epoch 595, loss: 0.015290428884327412\n",
      "In epoch 600, loss: 0.01511281169950962\n",
      "AUC 0.8390754924642304\n",
      "In epoch 605, loss: 0.014796729199588299\n",
      "In epoch 610, loss: 0.014478972181677818\n",
      "In epoch 615, loss: 0.014306951314210892\n",
      "In epoch 620, loss: 0.014080612920224667\n",
      "In epoch 625, loss: 0.013800605200231075\n",
      "In epoch 630, loss: 0.013590723276138306\n",
      "In epoch 635, loss: 0.013443822972476482\n",
      "In epoch 640, loss: 0.013195783831179142\n",
      "In epoch 645, loss: 0.012962485663592815\n",
      "In epoch 650, loss: 0.012767541222274303\n",
      "In epoch 655, loss: 0.0126060014590621\n",
      "In epoch 660, loss: 0.012446314096450806\n",
      "In epoch 665, loss: 0.01227777823805809\n",
      "In epoch 670, loss: 0.01215587928891182\n",
      "In epoch 675, loss: 0.01199996005743742\n",
      "In epoch 680, loss: 0.011835810728371143\n",
      "In epoch 685, loss: 0.01168677769601345\n",
      "In epoch 690, loss: 0.01154637522995472\n",
      "In epoch 695, loss: 0.011437698267400265\n",
      "In epoch 700, loss: 0.011324023827910423\n",
      "AUC 0.8390305698434446\n",
      "In epoch 705, loss: 0.011215612292289734\n",
      "In epoch 710, loss: 0.011102164164185524\n",
      "In epoch 715, loss: 0.01098333764821291\n",
      "In epoch 720, loss: 0.010876283049583435\n",
      "In epoch 725, loss: 0.010774240829050541\n",
      "In epoch 730, loss: 0.010694806464016438\n",
      "In epoch 735, loss: 0.010608461685478687\n",
      "In epoch 740, loss: 0.010529598221182823\n",
      "In epoch 745, loss: 0.010445836931467056\n",
      "In epoch 750, loss: 0.010361656546592712\n",
      "In epoch 755, loss: 0.010279367677867413\n",
      "In epoch 760, loss: 0.010198228992521763\n",
      "In epoch 765, loss: 0.010105952620506287\n",
      "In epoch 770, loss: 0.010026276111602783\n",
      "In epoch 775, loss: 0.009951179847121239\n",
      "In epoch 780, loss: 0.009879800491034985\n",
      "In epoch 785, loss: 0.009808376431465149\n",
      "In epoch 790, loss: 0.009742313995957375\n",
      "In epoch 795, loss: 0.00968689750880003\n",
      "In epoch 800, loss: 0.009613304398953915\n",
      "AUC 0.839378270928326\n",
      "In epoch 805, loss: 0.009542213752865791\n",
      "In epoch 810, loss: 0.00945847574621439\n",
      "In epoch 815, loss: 0.009367082267999649\n",
      "In epoch 820, loss: 0.00928469654172659\n",
      "In epoch 825, loss: 0.009197278879582882\n",
      "In epoch 830, loss: 0.00912313349545002\n",
      "In epoch 835, loss: 0.009079435840249062\n",
      "In epoch 840, loss: 0.009000956080853939\n",
      "In epoch 845, loss: 0.008943223394453526\n",
      "In epoch 850, loss: 0.008886830881237984\n",
      "In epoch 855, loss: 0.00884003285318613\n",
      "In epoch 860, loss: 0.008787792176008224\n",
      "In epoch 865, loss: 0.00873724278062582\n",
      "In epoch 870, loss: 0.00869304221123457\n",
      "In epoch 875, loss: 0.008651753887534142\n",
      "In epoch 880, loss: 0.008614352904260159\n",
      "In epoch 885, loss: 0.008580665104091167\n",
      "In epoch 890, loss: 0.008539845235645771\n",
      "In epoch 895, loss: 0.008489090949296951\n",
      "In epoch 900, loss: 0.008464628830552101\n",
      "AUC 0.8388275195974935\n",
      "In epoch 905, loss: 0.008444881066679955\n",
      "In epoch 910, loss: 0.008405339904129505\n",
      "In epoch 915, loss: 0.008366044610738754\n",
      "In epoch 920, loss: 0.00835332740098238\n",
      "In epoch 925, loss: 0.00833802204579115\n",
      "In epoch 930, loss: 0.008278698660433292\n",
      "In epoch 935, loss: 0.008232253603637218\n",
      "In epoch 940, loss: 0.008219173178076744\n",
      "In epoch 945, loss: 0.008239557035267353\n",
      "In epoch 950, loss: 0.008246279321610928\n",
      "In epoch 955, loss: 0.008112018927931786\n",
      "In epoch 960, loss: 0.00814405269920826\n",
      "In epoch 965, loss: 0.008259974420070648\n",
      "In epoch 970, loss: 0.008032781071960926\n",
      "In epoch 975, loss: 0.008107728324830532\n",
      "In epoch 980, loss: 0.007974526844918728\n",
      "In epoch 985, loss: 0.00806821696460247\n",
      "In epoch 990, loss: 0.007926683872938156\n",
      "In epoch 995, loss: 0.00814905297011137\n",
      "In epoch 1000, loss: 0.007864567451179028\n",
      "AUC 0.8372956582287011\n"
     ]
    }
   ],
   "source": [
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(1001):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata[\"x\"])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "    # ----------- 5. check results ------------------------ #\n",
    "    if e % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            pos_score = pred(test_pos_g, h)\n",
    "            neg_score = pred(test_neg_g, h)\n",
    "            print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor model\n",
    "actor = Actor(root='data/Actor', transform=NormalizeFeatures())\n",
    "\n",
    "# Cora model:\n",
    "train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = create_train_test_split_edge(actor[0])\n",
    "\n",
    "model = GraphSAGE(train_g.ndata[\"x\"].shape[1], 16)\n",
    "pred = DotPredictor()\n",
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.705256998538971\n",
      "In epoch 5, loss: 0.6895120143890381\n",
      "In epoch 10, loss: 0.6700869798660278\n",
      "In epoch 15, loss: 0.6250309348106384\n",
      "In epoch 20, loss: 0.5564284920692444\n",
      "In epoch 25, loss: 0.5070099830627441\n",
      "In epoch 30, loss: 0.4828763008117676\n",
      "In epoch 35, loss: 0.4588315188884735\n",
      "In epoch 40, loss: 0.43904662132263184\n",
      "In epoch 45, loss: 0.4211689829826355\n",
      "In epoch 50, loss: 0.4033316969871521\n",
      "In epoch 55, loss: 0.38598501682281494\n",
      "In epoch 60, loss: 0.36767756938934326\n",
      "In epoch 65, loss: 0.3482709527015686\n",
      "In epoch 70, loss: 0.327592670917511\n",
      "In epoch 75, loss: 0.30565235018730164\n",
      "In epoch 80, loss: 0.2826615273952484\n",
      "In epoch 85, loss: 0.2592843472957611\n",
      "In epoch 90, loss: 0.23580880463123322\n",
      "In epoch 95, loss: 0.21299971640110016\n",
      "AUC 0.8322247927944115\n"
     ]
    }
   ],
   "source": [
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata[\"x\"])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
